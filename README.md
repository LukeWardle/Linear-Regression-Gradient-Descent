# Linear Regression with Gradient Descent

This Jupyter Notebook demonstrates a mini project on implementing Linear Regression using Gradient Descent. The project is designed to deepen the understanding of fundamental machine learning concepts by showcasing the following:

- **Gradient Descent Implementation:**  
  Learn how to iteratively update model parameters (Î¸) to minimize the Mean Squared Error (MSE) between predicted and actual values.

- **Hyperparameter Tuning:**  
  Experiment with different learning rates and iteration counts to observe their impact on the convergence behavior of the model. Visualizations of the cost function over iterations provide insight into how the model learns.

- **Cost Function Visualization:**  
  See how the cost (MSE) decreases over iterations, which helps in understanding the stability and efficiency of the optimization process.

This project builds upon previous work, such as the [California Housing Price Prediction - Linear Regression](https://github.com/LukeWardle/California-Housing-Regression) project, by taking a deeper dive into model optimization and hyperparameter experimentation.

**Key Takeaways:**
- Grasping the core principles behind gradient descent.
- Understanding how learning rates and the number of iterations affect model convergence.
- Gaining hands-on experience with visualizing model performance and error convergence.

Feel free to explore, modify, and expand this notebook to further your understanding of these essential machine learning techniques.
